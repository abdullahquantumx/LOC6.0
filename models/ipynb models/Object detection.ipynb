{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":8,"metadata":{"id":"tt6nGJ71lWyk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710007105580,"user_tz":-330,"elapsed":9849,"user":{"displayName":"Abdullah Siddiqui","userId":"05024558492145642703"}},"outputId":"ba99f993-4d56-48ae-ff9f-28fbb6a6b1c9"},"outputs":[{"output_type":"stream","name":"stderr","text":["YOLOv5 ðŸš€ v7.0-290-gb2ffe055 Python-3.10.12 torch-2.1.0+cu121 CPU\n"]},{"output_type":"stream","name":"stdout","text":["Setup complete âœ… (2 CPUs, 12.7 GB RAM, 27.3/107.7 GB disk)\n"]}],"source":["\n","!git clone https://github.com/ultralytics/yolov5  # clone\n","%cd yolov5\n","%pip install -qr requirements.txt comet_ml  # install\n","\n","import torch\n","import utils\n","display = utils.notebook_init()  # checks"]},{"cell_type":"code","source":["\n","!python detect.py --weights yolov5s.pt --img 640 --conf 0.25 --source data/images\n","# display.Image(filename='runs/detect/exp/zidane.jpg', width=600)"],"metadata":{"id":"Pfq0RQPlngz-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710007116443,"user_tz":-330,"elapsed":10886,"user":{"displayName":"Abdullah Siddiqui","userId":"05024558492145642703"}},"outputId":"f886dd33-b410-4f4b-f89d-3d97a34ca303"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mdetect: \u001b[0mweights=['yolov5s.pt'], source=data/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n","YOLOv5 ðŸš€ v7.0-290-gb2ffe055 Python-3.10.12 torch-2.1.0+cu121 CPU\n","\n","Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n","100% 14.1M/14.1M [00:00<00:00, 118MB/s]\n","\n","Fusing layers... \n","YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n","image 1/2 /content/yolov5/yolov5/data/images/bus.jpg: 640x480 4 persons, 1 bus, 494.0ms\n","image 2/2 /content/yolov5/yolov5/data/images/zidane.jpg: 384x640 2 persons, 2 ties, 472.2ms\n","Speed: 2.1ms pre-process, 483.1ms inference, 4.4ms NMS per image at shape (1, 3, 640, 640)\n","Results saved to \u001b[1mruns/detect/exp\u001b[0m\n"]}]},{"cell_type":"code","source":["# Download COCO val\n","torch.hub.download_url_to_file('https://ultralytics.com/assets/coco2017val.zip', 'tmp.zip')  # download (780M - 5000 images)\n","!unzip -q tmp.zip -d ../datasets && rm tmp.zip  # unzip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XQmyUa9vr-Jn","executionInfo":{"status":"ok","timestamp":1710007134211,"user_tz":-330,"elapsed":17794,"user":{"displayName":"Abdullah Siddiqui","userId":"05024558492145642703"}},"outputId":"d959bfd7-c249-4718-dcb3-9f96f50fd9bf"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 780M/780M [00:05<00:00, 150MB/s]\n"]}]},{"cell_type":"code","source":["\n","# Validate YOLOv5s on COCO val\n","!python val.py --weights yolov5s.pt --data coco.yaml --img 640 --half\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FYOIdzZOsZVH","executionInfo":{"status":"ok","timestamp":1710007146190,"user_tz":-330,"elapsed":12005,"user":{"displayName":"Abdullah Siddiqui","userId":"05024558492145642703"}},"outputId":"2b5b4bbb-d1c4-43f2-850b-87a6b1b21290"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mdata=/content/yolov5/yolov5/data/coco.yaml, weights=['yolov5s.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=True, project=runs/val, name=exp, exist_ok=False, half=True, dnn=False\n","YOLOv5 ðŸš€ v7.0-290-gb2ffe055 Python-3.10.12 torch-2.1.0+cu121 CPU\n","\n","Fusing layers... \n","YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n","\u001b[34m\u001b[1mval: \u001b[0mScanning /content/yolov5/datasets/coco/val2017... 4952 images, 48 backgrounds, 0 corrupt: 100% 5000/5000 [00:03<00:00, 1325.94it/s]\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/yolov5/datasets/coco/val2017.cache\n","                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0% 0/157 [00:00<?, ?it/s]\n","Traceback (most recent call last):\n","  File \"/content/yolov5/yolov5/val.py\", line 438, in <module>\n","    main(opt)\n","  File \"/content/yolov5/yolov5/val.py\", line 409, in main\n","    run(**vars(opt))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/content/yolov5/yolov5/val.py\", line 236, in run\n","    preds, train_out = model(im) if compute_loss else (model(im, augment=augment), None)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/yolov5/yolov5/models/common.py\", line 650, in forward\n","    y = self.model(im, augment=augment, visualize=visualize) if augment or visualize else self.model(im)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/yolov5/yolov5/models/yolo.py\", line 263, in forward\n","    return self._forward_once(x, profile, visualize)  # single-scale inference, train\n","  File \"/content/yolov5/yolov5/models/yolo.py\", line 167, in _forward_once\n","    x = m(x)  # run\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/content/yolov5/yolov5/models/common.py\", line 90, in forward_fuse\n","    return self.act(self.conv(x))\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n","    return self._call_impl(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n","    return forward_call(*args, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 460, in forward\n","    return self._conv_forward(input, self.weight, self.bias)\n","  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\n","    return F.conv2d(input, weight, bias, self.stride,\n","RuntimeError: \"slow_conv2d_cpu\" not implemented for 'Half'\n"]}]},{"cell_type":"code","source":["\n","# # YOLOv5 PyTorch HUB Inference (DetectionModels only)\n","# import torch\n","\n","# # Load YOLOv5 model\n","# model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True, trust_repo=True)\n","\n","# # Path to the image\n","# im = '/content/tr3.jpeg'\n","\n","# # Perform inference\n","# results = model(im)\n","\n","# # Display the results\n","# results.show()\n","\n","# # Save the results\n","# results.save()\n","\n","# # Count the number of objects detected\n","# object_count = len(results.pandas().xyxy[0])\n","\n","# print(\"Number of objects detected:\", object_count)\n"],"metadata":{"id":"3vJRrfn0seim","executionInfo":{"status":"ok","timestamp":1710007146190,"user_tz":-330,"elapsed":32,"user":{"displayName":"Abdullah Siddiqui","userId":"05024558492145642703"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["import torch\n","\n","# Load YOLOv5 model\n","model = torch.hub.load('ultralytics/yolov5', 'yolov5s', force_reload=True, trust_repo=True)\n","\n","# Path to the image\n","im = '/content/tr3.jpeg'\n","\n","# Perform inference\n","results = model(im)\n","\n","# Define the object classes you want to count\n","object_classes = ['refrigerator', 'chair', 'bed', 'dining table', 'tv', 'bottle', 'sink', 'remote', 'bowl', 'cup', 'spoon', 'fork','wine glass','glass','broccoli']\n","\n","# Define probability threshold\n","prob_threshold = 0.2\n","\n","# Count the number of each type of object detected\n","object_counts = {obj_class: 0 for obj_class in object_classes}\n","\n","for detection in results.pred[0]:\n","    prob = detection[4].item()  # Probability of detection\n","    if prob >= prob_threshold:\n","        obj_class = results.names[int(detection[-1])]\n","        if obj_class in object_classes:\n","            object_counts[obj_class] += 1\n","\n","# Display the count for each type of object\n","for obj_class, count in object_counts.items():\n","    print(f\"Number of {obj_class}: {count}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NmtABi_g8V_q","executionInfo":{"status":"ok","timestamp":1710007147424,"user_tz":-330,"elapsed":1265,"user":{"displayName":"Abdullah Siddiqui","userId":"05024558492145642703"}},"outputId":"5d97e7e9-e498-43de-f3b0-bfe7a4fdf5c1"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to /root/.cache/torch/hub/master.zip\n","YOLOv5 ðŸš€ v7.0-290-gb2ffe055 Python-3.10.12 torch-2.1.0+cu121 CPU\n","\n","Fusing layers... \n","YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n","Adding AutoShape... \n"]},{"output_type":"stream","name":"stdout","text":["Number of refrigerator: 0\n","Number of chair: 0\n","Number of bed: 0\n","Number of dining table: 0\n","Number of tv: 0\n","Number of bottle: 0\n","Number of sink: 0\n","Number of remote: 0\n","Number of bowl: 0\n","Number of cup: 0\n","Number of spoon: 0\n","Number of fork: 0\n","Number of wine glass: 2\n","Number of glass: 0\n","Number of broccoli: 3\n"]}]},{"cell_type":"code","source":["import pickle\n","\n","# Assuming 'model' is your trained YOLOv5 model\n","\n","# Define the path where you want to save the model\n","model_path = '/content/ObjectDetection.pkl'\n","\n","# Save the model\n","with open(model_path, 'wb') as f:\n","    pickle.dump(model, f)\n","\n","print(f\"Model saved to '{model_path}'\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"42b-pc0ROtvH","executionInfo":{"status":"ok","timestamp":1710007147424,"user_tz":-330,"elapsed":7,"user":{"displayName":"Abdullah Siddiqui","userId":"05024558492145642703"}},"outputId":"16acc0cd-2fdb-493f-84ab-454e151a10ad"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Model saved to '/content/ObjectDetection.pkl'\n"]}]}]}